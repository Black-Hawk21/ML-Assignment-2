# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1py27xzaspTwULB8Q18YCeEYXB5KivtG-
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input

data = pd.read_csv('main.csv')
ID = data['trip_id']

ID

def load_and_preprocess_data(filepath):
    # 1. Load Data
    print("Loading data...")
    df = pd.read_csv(filepath)

    # 2. Basic Cleaning
    # Drop ID column as it adds no predictive value
    if 'trip_id' in df.columns:
        df = df.drop('trip_id', axis=1)

    # Drop duplicates if any
    df = df.drop_duplicates()

    # Separating Features and Target
    # Based on snippets, 'cost_category' is the target
    target_col = 'spend_category'

    # Drop rows where target is missing (if any)
    df = df.dropna(subset=[target_col])

    X = df.drop(target_col, axis=1)
    y = df[target_col]

    # 3. Define Column Types for Processing
    # We identify numerical and categorical columns automatically
    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns

    print(f"Numerical columns: {list(numerical_cols)}")
    print(f"Categorical columns: {list(categorical_cols)}")

    # 4. Create Preprocessing Pipelines

    # Numerical Pipeline: Impute missing values with Median -> Scale Standard Deviation
    # NNs require scaled data (mean=0, var=1) for faster convergence
    num_pipeline = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    # Categorical Pipeline: Impute missing with 'missing' -> OneHotEncode
    # handle_unknown='ignore' ensures the model works even if new categories appear in production
    cat_pipeline = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    # Combine pipelines
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', num_pipeline, numerical_cols),
            ('cat', cat_pipeline, categorical_cols)
        ]
    )

    # 5. Apply Preprocessing
    print("Fitting preprocessor on training data...")
    # FIT happens here
    X_processed = preprocessor.fit_transform(X)

    # 6. Encode Target
    # Neural Networks output probabilities, so classes need to be 0, 1, 2...
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    # Convert to categorical (one-hot for target) if using categorical_crossentropy
    num_classes = len(np.unique(y_encoded))
    y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)

    # 7. Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_categorical, test_size=0.2, random_state=42
    )

    print(f"Data ready. Input shape: {X_train.shape}, Classes: {num_classes}")

    # Return preprocessor so it can be used on test data later
    return X_processed, y_categorical, num_classes, X_processed.shape[1], preprocessor

def build_neural_network(input_dim, num_classes):
    """
    Builds a standard Feed-Forward Neural Network for classification.
    """
    model = Sequential([
        # Input Layer
        Input(shape=(input_dim,)),

        # Hidden Layer 1: High number of neurons to capture patterns
        Dense(128, activation='relu'),
        Dropout(0.3), # Regularization to prevent overfitting

        # Hidden Layer 2
        Dense(256, activation='relu'),
        Dropout(0.2),

        # Hidden Layer 3
        Dense(1024, activation='relu'),

        # Output Layer: Softmax for multi-class classification
        Dense(num_classes, activation='softmax')
    ])

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

csv_file = 'main.csv'
try:
    # Run Pipeline
    X_train, y_train, num_classes, input_dim, preprocessor = load_and_preprocess_data(csv_file)

    # Build Model
    model = build_neural_network(input_dim, num_classes)
    model.summary()

    # Train Model
    print("Training Neural Network...")
    history = model.fit(
        X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_split=0.2,
        verbose=1
    )

except FileNotFoundError:
    print(f"Error: {csv_file} not found. Please check the file path.")
except Exception as e:
    print(f"An error occurred: {e}")

def preprocess_new_data(filepath, preprocessor):
    """
    Preprocessing for NEW/TEST data.
    Crucial: Uses the preprocessor fitted on TRAINING data.
    """
    print(f"Loading new data from {filepath}...")
    df = pd.read_csv(filepath)

    # 2. Basic Cleaning (Mirroring train steps)
    if 'trip_id' in df.columns:
        df = df.drop('trip_id', axis=1)

    # Remove Target if present (to simulate blind prediction)
    if 'spend_category' in df.columns:
        df = df.drop('spend_category', axis=1)

    # 3. Apply Preprocessing (TRANSFORM ONLY)
    # Do NOT call fit_transform here.
    print("Transforming new data...")
    X_new_processed = preprocessor.transform(df)

    print(f"New data ready. Shape: {X_new_processed.shape}")
    return X_new_processed

test_file = "test.csv"
X_new = preprocess_new_data(test_file, preprocessor)
predictions = model.predict(X_new)

submission = pd.DataFrame(columns=['trip_id', 'spend_category'])

# 1. Load original test.csv to get Tour_ID
# Assuming test_file variable is available from previous cells, which points to 'test.csv'
df_test_original = pd.read_csv(test_file)
test_tour_ids = df_test_original['trip_id']

# 2. Use the 'predictions' generated from X_new in the previous cell
# If 'predictions' variable is not available, you would run: predictions = model.predict(X_new)

# Convert probabilities to class labels
y_pred_classes = np.argmax(predictions, axis=1)

# Assign Tour_ID and predicted Cost_Category
submission['trip_id'] = test_tour_ids
submission['spend_category'] = y_pred_classes

# Save to submission.csv
submission.to_csv('submission.csv', index=False)
print("Submission file 'submission.csv' created successfully!")
print(submission.head())